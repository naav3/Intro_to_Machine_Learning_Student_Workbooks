{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 13:58:14.664755: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "# keras module for building LSTM \n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import keras\n",
    "import tensorflow as tf\n",
    "if IN_COLAB:\n",
    "  !pip install Keras-Preprocessing\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9924</th>\n",
       "      <td>YOU ONLY LOSE IF YOU SELL AT A LOSS ---- HOLD</td>\n",
       "      <td>1</td>\n",
       "      <td>l71729</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>8</td>\n",
       "      <td>1.611880e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-29 02:22:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30936</th>\n",
       "      <td>GME long!</td>\n",
       "      <td>8</td>\n",
       "      <td>ldkrpa</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>7</td>\n",
       "      <td>1.612598e+09</td>\n",
       "      <td>GME long is the way! I’m not pushing anyone to...</td>\n",
       "      <td>2021-02-06 09:48:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27147</th>\n",
       "      <td>It was a win.</td>\n",
       "      <td>10</td>\n",
       "      <td>ld042l</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.612531e+09</td>\n",
       "      <td>Lots of doom and fingers in butts the last cou...</td>\n",
       "      <td>2021-02-05 15:13:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31508</th>\n",
       "      <td>We can get back on the hedge funds by uniting ...</td>\n",
       "      <td>0</td>\n",
       "      <td>ldu6am</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>45</td>\n",
       "      <td>1.612634e+09</td>\n",
       "      <td>*SILVER**</td>\n",
       "      <td>2021-02-06 19:51:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10100</th>\n",
       "      <td>Never invested before</td>\n",
       "      <td>1</td>\n",
       "      <td>l7153e</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>9</td>\n",
       "      <td>1.611880e+09</td>\n",
       "      <td>I’m in the US. How do I buy amc stock? Now?</td>\n",
       "      <td>2021-01-29 02:20:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34769</th>\n",
       "      <td>To all the boys holding GME. This video remind...</td>\n",
       "      <td>13</td>\n",
       "      <td>lnwhcg</td>\n",
       "      <td>https://youtu.be/6o148ck5OdQ</td>\n",
       "      <td>13</td>\n",
       "      <td>1.613815e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-20 12:00:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29788</th>\n",
       "      <td>Lets assume there is a second squeeze</td>\n",
       "      <td>20</td>\n",
       "      <td>ldg6u4</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>29</td>\n",
       "      <td>1.612585e+09</td>\n",
       "      <td>Okay assuming there are still enough shorts ar...</td>\n",
       "      <td>2021-02-06 06:16:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50246</th>\n",
       "      <td>$BCRX Fundamentally sound with the potential f...</td>\n",
       "      <td>0</td>\n",
       "      <td>o6rbgf</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>24</td>\n",
       "      <td>1.624529e+09</td>\n",
       "      <td>Hello everyone today I am going to try and con...</td>\n",
       "      <td>2021-06-24 13:07:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>Palantir Technologies and Rio Tinto Sign Multi...</td>\n",
       "      <td>4</td>\n",
       "      <td>l6wugu</td>\n",
       "      <td>https://www.businesswire.com/news/home/2021012...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.611870e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 23:40:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52789</th>\n",
       "      <td>It just makes SENS...</td>\n",
       "      <td>87</td>\n",
       "      <td>p01sg9</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>61</td>\n",
       "      <td>1.628372e+09</td>\n",
       "      <td>Alright apes, so with FDA approval likely upo...</td>\n",
       "      <td>2021-08-08 00:29:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  score      id  \\\n",
       "9924       YOU ONLY LOSE IF YOU SELL AT A LOSS ---- HOLD      1  l71729   \n",
       "30936                                          GME long!      8  ldkrpa   \n",
       "27147                                      It was a win.     10  ld042l   \n",
       "31508  We can get back on the hedge funds by uniting ...      0  ldu6am   \n",
       "10100                              Never invested before      1  l7153e   \n",
       "34769  To all the boys holding GME. This video remind...     13  lnwhcg   \n",
       "29788              Lets assume there is a second squeeze     20  ldg6u4   \n",
       "50246  $BCRX Fundamentally sound with the potential f...      0  o6rbgf   \n",
       "1142   Palantir Technologies and Rio Tinto Sign Multi...      4  l6wugu   \n",
       "52789                              It just makes SENS...     87  p01sg9   \n",
       "\n",
       "                                                     url  comms_num  \\\n",
       "9924   https://www.reddit.com/r/wallstreetbets/commen...          8   \n",
       "30936  https://www.reddit.com/r/wallstreetbets/commen...          7   \n",
       "27147  https://www.reddit.com/r/wallstreetbets/commen...          1   \n",
       "31508  https://www.reddit.com/r/wallstreetbets/commen...         45   \n",
       "10100  https://www.reddit.com/r/wallstreetbets/commen...          9   \n",
       "34769                       https://youtu.be/6o148ck5OdQ         13   \n",
       "29788  https://www.reddit.com/r/wallstreetbets/commen...         29   \n",
       "50246  https://www.reddit.com/r/wallstreetbets/commen...         24   \n",
       "1142   https://www.businesswire.com/news/home/2021012...          1   \n",
       "52789  https://www.reddit.com/r/wallstreetbets/commen...         61   \n",
       "\n",
       "            created                                               body  \\\n",
       "9924   1.611880e+09                                                NaN   \n",
       "30936  1.612598e+09  GME long is the way! I’m not pushing anyone to...   \n",
       "27147  1.612531e+09  Lots of doom and fingers in butts the last cou...   \n",
       "31508  1.612634e+09                                          *SILVER**   \n",
       "10100  1.611880e+09        I’m in the US. How do I buy amc stock? Now?   \n",
       "34769  1.613815e+09                                                NaN   \n",
       "29788  1.612585e+09  Okay assuming there are still enough shorts ar...   \n",
       "50246  1.624529e+09  Hello everyone today I am going to try and con...   \n",
       "1142   1.611870e+09                                                NaN   \n",
       "52789  1.628372e+09   Alright apes, so with FDA approval likely upo...   \n",
       "\n",
       "                 timestamp  \n",
       "9924   2021-01-29 02:22:31  \n",
       "30936  2021-02-06 09:48:56  \n",
       "27147  2021-02-05 15:13:17  \n",
       "31508  2021-02-06 19:51:30  \n",
       "10100  2021-01-29 02:20:35  \n",
       "34769  2021-02-20 12:00:53  \n",
       "29788  2021-02-06 06:16:27  \n",
       "50246  2021-06-24 13:07:46  \n",
       "1142   2021-01-28 23:40:59  \n",
       "52789  2021-08-08 00:29:01  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Data\n",
    "train_text_file = keras.utils.get_file('train_text.txt', 'https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/reddit_wsb.csv')\n",
    "train_text = pd.read_csv(train_text_file)\n",
    "train_text.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text\n",
    "\n",
    "We can clean and prep our text here. The data cleanup we need is to:\n",
    "<ul>\n",
    "<li> Remove punctuation.\n",
    "<li> Tokenize the text, as we did previously in NLP processing. \n",
    "<li> <b>Generate sequences of tokens.</b> This is the key to the LSTM model, we are structuring the data to be a sequence of tokens. Our model will attempt to predict the next token, which in this case is the next word in the sentence.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = 1000\n",
    "OUTPUT_LENGTH = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/px/vhm_920n7zx2wvqq_ht0q5tm0000gp/T/ipykernel_50997/955513030.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  strip_punct = train_text[\"body\"].dropna().str.replace('[{}]'.format(string.punctuation), '')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/vhm_920n7zx2wvqq_ht0q5tm0000gp/T/ipykernel_50997/955513030.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mstrip_punct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[{}]'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0minp_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sequence_of_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrip_punct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0minp_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/px/vhm_920n7zx2wvqq_ht0q5tm0000gp/T/ipykernel_50997/955513030.py\u001b[0m in \u001b[0;36mget_sequence_of_tokens\u001b[0;34m(corpus, tokenizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minput_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtoken_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mn_gram_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \"\"\"\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences_generator\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                 \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnum_words\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_sequence_of_tokens(corpus, tokenizer):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "\n",
    "strip_punct = train_text[\"body\"].dropna().str.replace('[{}]'.format(string.punctuation), '')\n",
    "inp_seq, total_words = get_sequence_of_tokens(strip_punct, Tokenizer())\n",
    "inp_seq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Prep - Padding and Targets\n",
    "\n",
    "We also need to take the sequences and pad them, or make them all the same length. We will also create the targets - the next word in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_seq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Now we model. The data that we made mirrors the construction of a sentence.\n",
    "<ul>\n",
    "<li> X features - the sentence up to this point. \n",
    "<li> Y target - the word(s) that should come next. \n",
    "</ul>\n",
    "\n",
    "So, the model is effectively working to generate text just like a time series model works to predict the next value in a sequence of stock prices or hourly temperature. We train the model on, hopefully a large number of senteneces, where is sees many examples of \"here are some words\" (X values) and \"here is the next word\" (Y value). If we give it lots and lots of that training data, it should become better and better at determining what should come next, given the existing sentence. \n",
    "\n",
    "To do this well, we'd need a lot more data than we have, and much more time to train. We'd want to give the model enough data so that it can see lots and lots of examples of the same word in different contexts, and of similar contexts with different words. The patterns of language are really complex, so we need data that provides enough variation to demonstrate the patterns. \n",
    "\n",
    "The model is wrapped in a little function, so we can make a model to output a different number of words with more convenience.\n",
    "\n",
    "#### Embedding Layer\n",
    "\n",
    "We also use an embedding layer here, which accepts our enocoded inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, output_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layers - LSTM Layer\n",
    "    model.add(LSTM(100, return_sequences = True))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, OUTPUT_LENGTH)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "model.fit(predictors, label, epochs=100, verbose=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "We can create a little function to generate text. We can give it a seed text, and it will generate text based on that. We can also give it a number of words to generate, and it will generate that many words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print (generate_text(\"united states\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"preident trump\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"donald trump\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"india and china\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"new york\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"science and technology\", 5, model, max_sequence_len))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
